{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xclim: Software ecosystem for climate services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "List authors, their current affiliations,  up-to-date contact information, and ORCID if available. Add as many author lines as you need. \n",
    "\n",
    "- Author1 = {\"name\": \"Pascal Bourgault\", \"affiliation\": \"Ouranos Inc\", \"email\": \"bourgault.pascal@ouranos.ca\"}\n",
    "- Author2 = {\"name\": \"Travis Logan\", \"affiliation\": \"Ouranos Inc\", \"email\": \"logan.travis@ouranos.ca\", \"orcid\": \"orcid\"}\n",
    "- Author3 = {\"name\": \"David Huard\", \"affiliation\": \"Ouranos Inc\", \"email\": \"huard.david@ouranos.ca\", \"orcid\": \"orcid\"}\n",
    "- Author4 = {\"name\": \"Trevor J. Smith\", \"affiliation\": \"Ouranos Inc\", \"email\": \"smith.trevorj@ouranos.ca\", \"orcid\": \"orcid\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Template-Notebook-for-EarthCube---Long-Version\" data-toc-modified-id=\"Template-Notebook-for-EarthCube---Long-Version-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Template Notebook for EarthCube - Long Version</a></span><ul class=\"toc-item\"><li><span><a href=\"#Author(s)\" data-toc-modified-id=\"Author(s)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Author(s)</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Technical-contributions\" data-toc-modified-id=\"Technical-contributions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Technical contributions</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Methodology</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Work-In-Progress---improvements\" data-toc-modified-id=\"Work-In-Progress---improvements-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Work In Progress - improvements</a></span></li><li><span><a href=\"#Suggested-next-steps\" data-toc-modified-id=\"Suggested-next-steps-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Suggested next steps</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Acknowledgements</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-import\" data-toc-modified-id=\"Library-import-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library import</a></span></li><li><span><a href=\"#Local-library-import\" data-toc-modified-id=\"Local-library-import-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Local library import</a></span></li></ul></li><li><span><a href=\"#Parameter-definitions\" data-toc-modified-id=\"Parameter-definitions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameter definitions</a></span></li><li><span><a href=\"#Data-import\" data-toc-modified-id=\"Data-import-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data import</a></span></li><li><span><a href=\"#Data-processing-and-analysis\" data-toc-modified-id=\"Data-processing-and-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data processing and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-10-rules\" data-toc-modified-id=\"The-10-rules-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>The 10 rules</a></span></li><li><span><a href=\"#Using-notebook-template\" data-toc-modified-id=\"Using-notebook-template-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Using notebook template</a></span></li><li><span><a href=\"#Adding-table-of-contents\" data-toc-modified-id=\"Adding-table-of-contents-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Adding table of contents</a></span></li><li><span><a href=\"#Creating-Binder-and-Docker-for-your-notebook-repository\" data-toc-modified-id=\"Creating-Binder-and-Docker-for-your-notebook-repository-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Creating Binder and Docker for your notebook repository</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook presents the [`xclim`](https://xclim.readthedocs.io/en/stable/) python package and how it can be used to power climate analysis workflows. It goes in details on what tools xclim provides, how it can simplify and standardize climate analyses workflows. The notebook also demontrates how xclim can be used as a Web Processing Service ([WPS](http://opengeospatial.github.io/e-learning/wps/text/basic-main.html)) through [`finch`](https://pavics-sdi.readthedocs.io/projects/finch/en/latest/). The notebook is developped for Python 3.7+, but expects an special python environment and a running instance of finch. The configuration is optimized for running on `binder`.\n",
    "\n",
    "\n",
    "## Technical contributions\n",
    "- Development of a climate analysis library regrouping many different tools, with special attention to community standards (especially [CF](cfconventions.org/)) and computationnally efficient implementations.\n",
    "  * Climate indices computation in a standard manner with numerically efficient algorithms\n",
    "  * Ensemble tools for computing ensemble statistics and robustness\n",
    "  * Bias-adjustment algorithms\n",
    "  * Other tools (not shown in this notebook) (subsetting, spatial analogs, calendar conversions)\n",
    "- Development of a web service exposing climate analysis processes through OGC's WPS protocol\n",
    "\n",
    "\n",
    "## Methodology\n",
    "This notebook shows two examples of climate analysis workflows:\n",
    "\n",
    "1. Calculating climate indices on an ensemble of simulation data and computing ensemble statistics.\n",
    "2. Bias-adjustment of model data\n",
    "\n",
    "Part of the first workflow will be repeated, but this time dispatching the computation to a (locally running) web service (`finch`), simulating use of xclim's tools in the cloud.\n",
    "\n",
    "The examples will interleave code and markdown cells to describe what is being done. It will also include some  visualization using holoviews, in order to give a clearer idea of what was achieved.\n",
    "\n",
    "## Results\n",
    "This notebook demontrates how the use of xclim can help simplify and standardize climate analysis workflows. \n",
    "\n",
    "\n",
    "## Funding\n",
    "Include references to awards that supported this research. Add as many award references as you need.\n",
    "\n",
    "- Award1 = {\"agency\": \"US National Science Foundation\", \"award_code\": \"1928208\", \"award_URL\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1928208\"}\n",
    "- Award2 = {\"agency\": \"agency\", \"award_code\": \"award_code\", \"award_URL\": \"award_URL\"}\n",
    "- Award3 = ...\n",
    "\n",
    "## Keywords\n",
    "\n",
    "keywords=[\"climate\", \"xarray\", \"bias-adjustment\", \"wps\"]\n",
    "\n",
    "## Citation\n",
    "\n",
    "Bourgault, Pascal et al. Ouranos Inc, 2021. xclim: Software ecosystem for climate services. Accessed 2021/05/15 at https://github.com/Ouranosinc/xclim/blob/earthcube-nb/docs/notebooks/PB_01_xclim_Software_ecosystem_for_climate_services.ipynb\n",
    "\n",
    "\n",
    "## Suggested next steps\n",
    "\n",
    "After reading through this notebook, we recommend looking at xclim's documentation, especially the [examples](https://xclim.readthedocs.io/en/stable/notebooks/index.html) page, where other notebooks extend the examples shown here. Xclim offers other features that were not shown here, but might be of interest for at least part of the readers : [a spatial analogs submodule](https://xclim.readthedocs.io/en/stable/api.html#module-xclim.analog) and [internationalization utilities](https://xclim.readthedocs.io/en/stable/internationalization.html), among others. The [list of indicators](https://xclim.readthedocs.io/en/stable/indicators.html) is also a good stop. The list is constantly growing, including indicators ranging from the simple but universal [list from ECAD](https://xclim.readthedocs.io/en/stable/indicators.html#icclim-indices) to more complex and specialized one, like the [Fire Weather Indexes](https://xclim.readthedocs.io/en/stable/indices.html#fire-weather-indices-submodule).\n",
    "\n",
    "The web service demontrated here is part of the [`bird-house`](http://bird-house.github.io/) ecosystem and project. It is used by different projects, for example [PAVICS](https://pavics.ouranos.ca/), \n",
    "\n",
    "\n",
    "## Acknowledgements \n",
    "\n",
    "Include any relevant acknowledgements, apart from funding (which was in section 1.6)\n",
    "\n",
    "This notebook template extends the original notebook template provided with the jupytemplate extension [5]. It is a result of collaboration between the TAC Working Group and the EarthCube Office. \n",
    "\n",
    "The template is licensed under a <a href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclim as xc\n",
    "# Also import some submodules for direct access\n",
    "from xclim import ensembles  # Ensemble creation and statistics\n",
    "from xclim import sdba  # Bias-adjustment\n",
    "\n",
    "# Data manipulation\n",
    "import xarray as xr\n",
    "from xclim.testing import open_dataset\n",
    "\n",
    "# For interacting with finch WPS\n",
    "import birdy\n",
    "\n",
    "# Visualizations and display\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# For handling file paths\n",
    "from pathlib import Path\n",
    "\n",
    "# For masking warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Data importation will be done as we go in the notebook, but we present the datasets here. The first part will use bias-adjusted and downscaled data from Ouranos, a product call \"Generic Scenarios\". It regroups 11 simulations from different models, all produced in the CMIP5 project. Here we use realization following RCP 8.5. More information can be found on [this page](https://pavics.ouranos.ca/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs earthcube_data/*.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example, dealing with bias-adjustment, will use 2 sources. The model data comes from the [CanESM2 model](https://www.canada.ca/en/environment-climate-change/services/climate-change/science-research-data/modeling-projections-analysis/centre-modelling-analysis/models/second-generation-earth-system-model.html) of CCCma. The reference timeseries are extracted from the Adjusted and Homogenized Canadian Climate Data ([AHCCD](https://open.canada.ca/data/en/dataset/9c4ebc00-3ea4-4fe0-8bf2-66cfe1cddd1d)), from Environment and Climate Change Canada. Both datasets are distributed under the Open Government Licence (Canada). We chose three weather stations representative of a reasonable range of climatic conditions and extracted timeseries over those locations from the model data.\n",
    "\n",
    "Finally, the third section will use data extracted from ECMWF's [ERA5 reanalysis](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5). The dataset includes timeseries extracted on a few locations, for all variables that xclim could take as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n",
    "\n",
    "## 1. Climate indices computation and ensemble statistics\n",
    "\n",
    "A large part of climate data analytics is based on the computation and comparison of climate indices. Xclim's main goal is to make it easier for researchers to process climate data and calculate those indices. It does so by providing a large library of those climate indices, regrouped in categories (CMIP's \"realms\") for convenience, but also by providing tools for all the small tweaks and pre- and post-processing steps that such workflows need. \n",
    "\n",
    "As its name suggests, xclim is built upon [`xarray`](xarray.pydata.org/) which itself makes it easy to scale computations using [`dask`](https://dask.org). While many indices can be quite simple and straightforward functions, we aim to provide implementations well balanced between efficient performances and low code complexity, making it easy to understand and extend.\n",
    "\n",
    "In the following steps we will use xclim to: create an ensemble dataset, compute a few climate indices and calculate ensemble statistics on the results. \n",
    "\n",
    "### Creating the ensemble dataset \n",
    "\n",
    "In xclim, an \"ensemble dataset\" is simply an xarray `Dataset` where multiple realizations of the same climate have been concatenated along a \"realization\" dimension. In our case here, as presented above, the realization are in fact simulations from different models, but all have been bias-adjusted with the same reference. All ensemble-related functions are in the [`ensembles`](https://xclim.readthedocs.io/en/stable/api.html#module-xclim.ensembles) module.\n",
    "\n",
    "\n",
    "Given a list of files, creating the ensemble dataset is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of files in the `earthcube_data`:\n",
    "files = list(Path('earthcube_data').glob('*1950-2100.nc'))\n",
    "\n",
    "# Open files into the ensemble\n",
    "ens = ensembles.create_ensemble(files)\n",
    "ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how is this different from `xarray.concat`? In addition to being a bit more convenient, a main difference is the handling of incompatible calendars. Different models often use different [calendars](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.html#calendar) and this causes issues when merging/concatening their data in xarray. Xclim provides [`xclim.core.calendar.convert_calendar`](https://xclim.readthedocs.io/en/stable/api.html#xclim.core.calendar.convert_calendar) to handle this. For example, in this ensemble, we have a simulation of the `HadGEM2` model which uses the \"360_day\" calendar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds360 = xr.open_dataset('earthcube_data/HadGEM2-CC_rcp85_singlept_1950-2100.nc')\n",
    "ds360.time[59].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to the `noleap` calendar, see doc for details on arguments.\n",
    "ds365 = xc.core.calendar.convert_calendar(ds360, 'noleap', align_on='date')\n",
    "ds365.time[59].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conversion dropped the invalid days (for example: the 29th and 30th of February 1950) and convert the dtype of the time axis. This does alter the data, but it makes it possible to concatenate them using normal `xarray` tools.\n",
    "\n",
    "In `create_ensemble`, all members are converted to a common calendar. By default, it uses the normal numpy/pandas data type (a [range-limited](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timestamp-limitations) calendar equivalent to `proleptic_gregorian`), but for this model ensemble, it might be more useful to use `noleap` (all years have 365 days), as it is the calendar used in most models included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = ensembles.create_ensemble(files, calendar='noleap')\n",
    "ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute climate indices\n",
    "\n",
    "All climate indices exist in two version inside xclim, differentiated by the use of the words *indice* and *indicator*. However, for most users, the difference is not relevant as they will only use the latter, the *indicators*. We will use that word here from now on. \n",
    "\n",
    "Indicators are stored in `xclim.indicators.[realm]` and act as functions, taking `xarray.DataArray` as input variables and returning one (or more) `DataArray`. Most indicators also take keyword arguments to control various parameters. Under the hood, indicators are python objects, storing much information on what the computation does, its inputs and its outputs. For example, let's look at the `tropical_nights` indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = xc.indicators.atmos.tropical_nights\n",
    "\n",
    "# Print general information:\n",
    "print('Title: ', TN.title)\n",
    "print('General description: ', TN.abstract, '\\n')\n",
    "# Info on the inputs\n",
    "print('Inputs description:')\n",
    "pprint(TN.parameters)\n",
    "print()\n",
    "\n",
    "# Info on the output\n",
    "print('Output description:')\n",
    "pprint(TN.cf_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the indicator by calling it like a function with the variable and parameters. We will look at the _seasonal_ number of \"tropical nights\" with a threshold of 5°C. For frequency arguments, we use the same syntax as [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects). Here `QS-DEC` will resample to seasons of 3 months, starting in december (so DJF, MMA, JJA, SON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = xc.indicators.atmos.tropical_nights(tasmin=ens.tasmin, thresh='5 degC', freq='QS-DEC')\n",
    "display(out.data)\n",
    "out.isel(time=slice(0, 2)).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the many things that happened here:\n",
    "\n",
    "1. **Input checks**\n",
    "\n",
    "    Before the real computation, xclim performs some checks on the inputs. These can differ between indicators, but the most common are CF convention and input frequency checks. The former looks at the `standard_name` and `cell_methods` attributes of the input variable and raises warnings when they do not fit with what it expects. Here, the `standard_name` (\"air_temperature\") is correct, but the variable is missing the `cell_methods` attribute that would normally indicator that is indeed the *minimum* temperature within each days. \n",
    "    \n",
    "    The second type of checks looks at the data itself. Most indicators expect data at a **daily** frequency, and if the input has a different frequency, they will raise an error.\n",
    " \n",
    "\n",
    "2. **Unit handling**\n",
    "\n",
    "    Another check on the inputs is performed to ensure the correct units were passed. Above, in the `parameters` dictionary, we saw that `tropical_nights` expects `tasmin` and `thresh` to  both be in \"[temperature]\". In the function, conversion are made to ensure compatible units. Here, `tasmin` is in Kelvin and `thresh` was given in Celsius. Under the hood, xclim uses [`pint`](pint.readthedocs.io/) to handle this, but with small modifications made so that CF-compliant unit strings are understood.\n",
    "\n",
    "\n",
    "3. **Missing values handling**\n",
    "\n",
    "    After the computation, for indicators involving a resampling operation, periods where at least one value was missing (i.e. was a `np.NaN`) are masked out. Here the first period is masked for all realization since there was no data for the month of december 1949. This is the default behavior and it can be changed, as we will show further down.\n",
    "\n",
    "\n",
    "4. **Metadata formatting**\n",
    "\n",
    "   The output's attributes were formatted to include some information from the passed parameters. Notice how the description includes the given threshold and frequency (*Seasonal number of tropical nights [...]*).\n",
    "    \n",
    "\n",
    "5. **Lazy computing**\n",
    "\n",
    "   This is not a feature inherent to xclim, but, by default, `create_ensemble` will open the ensemble dataset using the `dask` backend, one \"chunk\" for each file. This means that all further calculations are _lazy_, i.e. python remembers the operations but doesn't perform them as long as the result is not requested. This is why we needed to call `.load()` to see the first values of the output. This behaviour from `xarray` is extremely useful for computationally-intensive workflows as it allows `dask` to efficiently manage the memory and cpu ressources. Xclim aims to implement its indicators in a lazy and efficient way.\n",
    "\n",
    "\n",
    "Let's compute a few indicators and merge them into another ensemble datasets. This time, we will change the missing values handling to a more relax method where periods are masked when at least 5% of the elements are missing. Moreover, we will set an option so that CF-checks warnings are ignored.\n",
    "\n",
    "If you want to know more about the indicators we are computing, you can use `help(indicator)` or `indicator?` (in ipython/jupyter) to print their docstring that includes most information shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options using a \"context\". Options set here are reset to their defaults as soon as we exit the \"with\" block\n",
    "with xc.set_options(cf_compliance='log', check_missing='pct', missing_options={'pct': {'tolerance': 0.05}}):\n",
    "    \n",
    "    # Intermediate computation : Approximate mean daily temp by taking the mean of tasmin and tasmax\n",
    "    tas = xc.indicators.atmos.tg(tasmin=ens.tasmin, tasmax=ens.tasmax)\n",
    "    \n",
    "    # First day below, first day of the year where tasmin goes below \"thresh\" for at least \"window\" days\n",
    "    # Note that we resample yearly, but starting in July, so that the full winter is included\n",
    "    out_fda = xc.indicators.atmos.first_day_below(tasmin=ens.tasmin, thresh='-5 degC', window=3, freq='AS-JUL')\n",
    "        \n",
    "    # Number frost days : total number of days where tasmin < 0 degC\n",
    "    out_nfd = xc.indicators.atmos.frost_days(tasmin=ens.tasmin, freq='AS-JUL')\n",
    "    \n",
    "    # Solid precipitation accumulation : total thickness of solid precipitation (estimated by pr when tas < 0°C)\n",
    "    out_spa = xc.indicators.atmos.solid_precip_accumulation(pr=ens.pr, tas=tas, freq='AS-JUL')\n",
    "    \n",
    "    # Cold spell freq : Number of spells where tas is under \"thresh\" for at least \"window\" days.\n",
    "    out_csf = xc.indicators.atmos.cold_spell_frequency(tas=tas, thresh='-10 degC', window=3, freq='AS-JUL')   \n",
    "    \n",
    "out = xr.merge([out_fda, out_nfd, out_spa, out_csf])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble statistics\n",
    "\n",
    "All this is quite cool, but we still have a large amount of data to manage. In order to get an idea of the climatic evolution of our indicators and of the model uncertainty associated with it, we can compute ensemble percentiles. Here we extract the 10th, 50th (median) and 90th percentile of the distribution made up of the 11 members of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_perc = ensembles.ensemble_percentiles(out, values=[10, 50, 90], split=False)\n",
    "out_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `first_day_below` indicator is problematic. The data is in a \"day of year\" (doy) format, an integer starting a 1 on January 1st and going to 365 on December 31st (in our *no leap* calendar). There might be years where the first day below 0°C happened _after_ December 31st, meaning that taking the percentiles directly will give incorrect results. The trick is simply to convert the \"day of year\" data to a format where the numerical order reflects the temporal order within each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"doys\" to the number of days elapsed since the time coordinate (which is 1st of July)\n",
    "fda_days_since = xc.core.calendar.doy_to_days_since(out.first_day_below)\n",
    "\n",
    "# Take percentiles now that the numerical ordering is correct\n",
    "fda_days_since_perc = ensembles.ensemble_percentiles(fda_days_since, values=[10, 50, 90], split=False)\n",
    "\n",
    "# Convert back to doys for meaningful values\n",
    "fda_doy_perc = xc.core.calendar.days_since_to_doy(fda_days_since_perc)\n",
    "\n",
    "# Replace the bad data with the good one\n",
    "out_perc['first_day_below'] = fda_doy_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! Lets enjoy our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    out_perc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, var in out_perc.data_vars.items():\n",
    "    plt.figure()\n",
    "    var.plot(hue='percentiles')\n",
    "    plt.title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the doy conversion trick did work for `first_day_below`, but plotting this kind of variable is not straightforward, and not the point of this notebook.\n",
    "\n",
    "## 2. Bias-adjustment\n",
    "\n",
    "Besides climate indicators, xclim comes with the `sdba` (statistical downscaling and bias-adjustment) module. The module tries to follow a \"modular\" approach instead of implementing full published methods as black box functions. It comes with some pre- and post-processing function in `sdba.processing` and a few adjustment objects in `sdba.adjustment`. Adjustment algorithms all conform to the train - adjust scheme, formalized within `Adjustment` classes. Given a reference time series (`ref`), historical simulations (`hist`) and simulations to be adjusted (`sim`), any bias-adjustment method would be applied by first estimating the adjustment factors between the historical simulation and the observations series, and then applying these factors to `sim`, which could be a future simulation.\n",
    "\n",
    "Most algorithms implemented also perform the adjustment separately on temporal groups. For example, for a Quantile Mapping method, one would compute quantiles and adjustment factors for each day of the year individually or within a moving window, so that seasonal variations do not interfere with the result.\n",
    "\n",
    "### Simple quantile mapping\n",
    "\n",
    "Let's start here with a simple Empirical Quantile Mapping adjustment for `pr`. Instead of adjusting a future period, we adjust the historical timeseries, so we can have a clear appreciation of the adjustment results.\n",
    "\n",
    "**Multiplicative** and **Additive** modes : In most of the litterature, bias-adjustment involving `pr` is done in a *multiplicative* mode, in opposition to the *additive* mode used for temperatures. In xclim.sdba, this is controlled with the `kind` keyword, which defaults to `'+'` (additive). This means that `sim` values will be multiplied to the adjustment factors in the case `kind='*'`, and added (or subtracted) with `'+'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are opening the datasets with xclim.testing.open_dataset\n",
    "# a convenience function for accessing xclim's test and example datasets, stored in a github repository.\n",
    "dsref = open_dataset('sdba/ahccd_1950-2013.nc')\n",
    "dssim = open_dataset('sdba/CanESM2_1950-2100.nc')\n",
    "\n",
    "# Extract 30 years of the daily data\n",
    "# The data is made of 3 timeseries, we take only one \"location\"\n",
    "# `ref` is given in Celsius, we must convert to Kelvin to fit with hist and sim\n",
    "ref = dsref.pr.sel(time=slice('1980', '2010'), location='Vancouver')\n",
    "ref = xc.core.units.convert_units_to(ref, 'mm/d')\n",
    "hist = dssim.pr.sel(time=slice('1980', '2010'), location='Vancouver')\n",
    "hist = xc.core.units.convert_units_to(hist, 'mm/d')\n",
    "\n",
    "# We want to group data on a day-of-year basis with a 31-day moving window\n",
    "group_doy_31 = sdba.Grouper('time.dayofyear', window=31)\n",
    "\n",
    "# Create the adjustment object\n",
    "EQM = sdba.EmpiricalQuantileMapping(nquantiles=15, group=group_doy_31, kind='*')\n",
    "\n",
    "# Train it\n",
    "EQM.train(ref, hist)\n",
    "\n",
    "# Adjust hist\n",
    "# And we estimate correct adjustment factors by interpolating linearly between quantiles\n",
    "scen = EQM.adjust(hist, interp='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the season cycle\n",
    "fig, ax = plt.subplots()\n",
    "ref.groupby('time.dayofyear').mean().plot(ax=ax, label='Reference')\n",
    "hist.groupby('time.dayofyear').mean().plot(ax=ax, label='Model - raw')\n",
    "scen.groupby('time.dayofyear').mean().plot(ax=ax, label='Model - adjusted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the trained adjustment data, we can access it with `EQM.ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQM.ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex adjustment workflows can be created using the multiple tools exposed by xclim in `sdba.processing` and `sdba.detrending`. A more complete description of those is a bit too complex for the purposes of this notebook, but we invite interested users to look at [the documentation](https://xclim.readthedocs.io/en/stable/notebooks/sdba.html).\n",
    "\n",
    "Let's make another simple example, using the `DetrendedQuantileMapping` method. Compared to the `EmpiricalQuantileMapping` method, this one will normalize and detrend the inputs before computing the adjustment factors. This ensures a better quantile-to-quantile comparison when the climate change signal is strong. In reality, the same steps could all be performed explicitly, making use of xclim's modularity, as is shown in the doc examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data.\n",
    "ref = xc.core.units.convert_units_to(\n",
    "    dsref.tasmax.sel(time=slice('1980', '2010'), location='Vancouver'),\n",
    "    'K'\n",
    ")\n",
    "hist = xc.core.units.convert_units_to(\n",
    "    dssim.tasmax.sel(time=slice('1980', '2010'), location='Vancouver'),\n",
    "    'K'\n",
    ")\n",
    "sim = xc.core.units.convert_units_to(\n",
    "    dssim.tasmax.sel(location='Vancouver'),\n",
    "    'K'\n",
    ")\n",
    "\n",
    "\n",
    "# Adjustment\n",
    "DQM = sdba.DetrendedQuantileMapping(nquantiles=15, group=group_doy_31, kind='+')\n",
    "DQM.train(ref, hist)\n",
    "\n",
    "scen = DQM.adjust(sim, interp='linear', detrend=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ref.groupby('time.dayofyear').mean().plot(ax=ax, label='Reference'),\n",
    "hist.groupby('time.dayofyear').mean().plot(ax=ax, label='Simulation - raw')\n",
    "scen.sel(time=slice('1981', '2010')).groupby('time.dayofyear').mean().plot(ax=ax, label='Simulation - adjusted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's compute the annual mean of the daily maximum temperature and plot the three timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xc.set_options(cf_compliance='log'):\n",
    "    ref_tg = xc.indicators.atmos.tx_mean(tasmax=ref)\n",
    "    sim_tg = xc.indicators.atmos.tx_mean(tasmax=sim)\n",
    "    scen_tg = xc.indicators.atmos.tx_mean(tasmax=scen)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ref_tg.plot(ax=ax, label='Reference')\n",
    "sim_tg.plot(ax=ax, label='Model - Raw')\n",
    "scen_tg.plot(ax=ax, label='Model - Adjusted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finch : xclim as a service\n",
    "\n",
    "This final part of the notebook demonstrates the use of `finch` to compute indicators over the web using the Web Processing Service protocol. When running this notebook on binder, a local server instance of finch is already running on port 5000. We will interact with it using [`birdy`](https://birdy.readthedocs.io/en/latest/) a lightweight WPS client built upon [`OWSLib`](https://geopython.github.io/OWSLib/).\n",
    "\n",
    "However, a limitation is of course that input files must be accessible through the web. For the following example, we will use a subset of the ERA5 reanalysis data, available on xclim's testdata Github repository. But instead of _downloading_ it like we did for the bias-adjustment examples, we will simply pass the \"raw\" url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdy import WPSClient\n",
    "\n",
    "wps = WPSClient('http://localhost:5000')\n",
    "dataurl = 'https://github.com/Ouranosinc/xclim-testdata/raw/main/ERA5/daily_surface_cancities_1990-1993.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Through birdy, the indicator call looks a lot like the one from xclim\n",
    "# but passing an url instead of an xarray object.\n",
    "# Also, the `variable` argument tells which variable from the dataset to use.\n",
    "response = wps.growing_degree_days(\n",
    "    dataurl,\n",
    "    thresh='5 degC',\n",
    "    freq='MS',\n",
    "    variable='tas'\n",
    ")\n",
    "\n",
    "\n",
    "response.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we received is a list of URLs pointing to our files. Birdy makes it easy to open links to datasets directly in xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = response.get(asobj=True).output_netcdf\n",
    "out.growing_degree_days.plot(hue='location');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "List relevant references.\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
